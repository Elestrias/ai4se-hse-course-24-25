{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c216b84e-98c0-4657-954e-298f1f9bee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (0.26.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: contractions in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Requirement already satisfied: inflect in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from inflect) (10.1.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from inflect) (4.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.11.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scikit-multilearn in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install contractions\n",
    "!pip install inflect\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-multilearn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d860421-b236-473d-91fb-376fe58cbf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import sys\n",
    "from random import randbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "799f6ee5-2be6-4d5d-8351-378fa7f8dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(str(\"./prepared-dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d94b0c-5576-4153-b863-66a8d98b1881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from transformers) (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: accelerate in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from accelerate) (2.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from accelerate) (0.26.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rus8-\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849508b6-91eb-40aa-8adf-5d8c5e37506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas\n",
    "from datasets import Dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report, f1_score\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, log_loss, accuracy_score, precision_score, recall_score\n",
    "\n",
    "GSCV_parameters = [{'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']},\n",
    "                   {'penalty': ['none', 'elasticnet', 'l1', 'l2']},\n",
    "                   {'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def classifier(dataset, model):\n",
    "    # Use real X and y from dataset\n",
    "    print(dataset)\n",
    "    dataset = dataset.to_pandas()\n",
    "    X = dataset[\"message\"]\n",
    "    y = dataset[\"is_toxic\"]\n",
    "    scores = []\n",
    "    matrices = []\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=46, shuffle=True)\n",
    "    if model == \"classic_ml\":\n",
    "        tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
    "        train_tfidf = tfidf_vec.fit_transform(x_train)\n",
    "        test_tfidf = tfidf_vec.transform(x_val)\n",
    "        print(tfidf_vec.get_feature_names_out()[:10])\n",
    "\n",
    "        res_model = model_classic_train(train_tfidf, y_train)\n",
    "        eval_res = eval_fun_br(test_tfidf, y_val, res_model)\n",
    "        print({'tn': eval_res[1][0, 0], 'fp': eval_res[1][0, 1],\n",
    "               'fn': eval_res[1][1, 0], 'tp': eval_res[1][1, 1]})\n",
    "\n",
    "\n",
    "    elif model in [\"roberta-base\", \"microsoft/codebert-base\"]:\n",
    "        x_tr, x_test, y_tr, y_test = train_test_split(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            test_size=0.8,\n",
    "            random_state=46,\n",
    "            shuffle=True)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model)\n",
    "        tf_bert = lambda examples: tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "        trd = Dataset.from_pandas(pd.DataFrame({\"text\": x_tr, \"labels\": y_tr})).map(tf_bert, batched=True)\n",
    "        tsd = Dataset.from_pandas(pd.DataFrame({\"text\": x_test, \"labels\": y_test})).map(tf_bert, batched=True)\n",
    "        vald = Dataset.from_pandas(pd.DataFrame({\"text\": x_val, \"labels\": y_val})).map(tf_bert, batched=True)\n",
    "        res_model = bert_model(dataset, trd, tsd, model)\n",
    "        metrics = res_model.evaluate(vald)\n",
    "        print(f\"MODEL \\\"{model}\\\" VALIDATION RESULTS\")\n",
    "        print(\"@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "        print(f\"Accuracy: {metrics['eval_accuracy']:.3f}\")\n",
    "        print(f\"Precision: {metrics['eval_precision']:.3f}\")\n",
    "        print(f\"Recall: {metrics['eval_recall']:.3f}\")\n",
    "        print(f\"F1 Score: {metrics['eval_f1']:.3f}\")\n",
    "        print(\"--------------------------\")\n",
    "\n",
    "    elif model == \"all\":\n",
    "        tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
    "        train_tfidf = tfidf_vec.fit_transform(x_train)\n",
    "        test_tfidf = tfidf_vec.transform(x_val)\n",
    "        print(tfidf_vec.get_feature_names_out()[:10])\n",
    "        res_model = model_classic_train(train_tfidf, y_train)\n",
    "\n",
    "        x_tr, x_test, y_tr, y_test = train_test_split(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            test_size=0.8,\n",
    "            random_state=46,\n",
    "            shuffle=True)\n",
    "\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        tf_bert = lambda examples: tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "        trd = Dataset.from_pandas(pd.DataFrame({\"text\": x_tr, \"labels\": y_tr})).map(tf_bert, batched=True)\n",
    "        tsd = Dataset.from_pandas(pd.DataFrame({\"text\": x_test, \"labels\": y_test})).map(tf_bert, batched=True)\n",
    "        vald_A = Dataset.from_pandas(pd.DataFrame({\"text\": x_val, \"labels\": y_val})).map(tf_bert, batched=True)\n",
    "\n",
    "        roberta = bert_model(dataset, trd, tsd, \"roberta-base\")\n",
    "\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        tf_bert = lambda examples: tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "        trd = Dataset.from_pandas(pd.DataFrame({\"text\": x_tr, \"labels\": y_tr})).map(tf_bert, batched=True)\n",
    "        tsd = Dataset.from_pandas(pd.DataFrame({\"text\": x_test, \"labels\": y_test})).map(tf_bert, batched=True)\n",
    "        vald_B = Dataset.from_pandas(pd.DataFrame({\"text\": x_val, \"labels\": y_val})).map(tf_bert, batched=True)\n",
    "\n",
    "        codebert = bert_model(dataset, trd, tsd, \"microsoft/codebert-base\")\n",
    "        print(\"MODELS FULL COMPARE\")\n",
    "        print(\"--------------------\")\n",
    "        print(\"Best Logistic Regression\")\n",
    "        print(eval_fun_br(test_tfidf, y_val, res_model))\n",
    "\n",
    "        metrics = roberta.evaluate(vald_A)\n",
    "        print(f\"MODEL \\\"ROBERTA\\\" RESULTS\")\n",
    "        print(\"@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "        print(f\"Accuracy: {metrics['eval_accuracy']:.3f}\")\n",
    "        print(f\"Precision: {metrics['eval_precision']:.3f}\")\n",
    "        print(f\"Recall: {metrics['eval_recall']:.3f}\")\n",
    "        print(f\"F1 Score: {metrics['eval_f1']:.3f}\")\n",
    "        print(\"--------------------------\")\n",
    "\n",
    "        metrics = codebert.evaluate(vald_B)\n",
    "        print(f\"MODEL \\\"CODEBERT\\\" RESULTS\")\n",
    "        print(\"@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "        print(f\"Accuracy: {metrics['eval_accuracy']:.3f}\")\n",
    "        print(f\"Precision: {metrics['eval_precision']:.3f}\")\n",
    "        print(f\"Recall: {metrics['eval_recall']:.3f}\")\n",
    "        print(f\"F1 Score: {metrics['eval_f1']:.3f}\")\n",
    "        print(\"--------------------------\")\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Invalid model name\")\n",
    "\n",
    "\n",
    "def model_classic_train(x_train, y_train):\n",
    "    lr_clf = LogisticRegression()\n",
    "    scoring = {\"Accuracy\": \"accuracy\", \"F1_Score\": \"f1\",\n",
    "               \"AUC\": \"roc_auc\",\n",
    "               \"NegLogLoss\": \"neg_log_loss\",\n",
    "               \"Precision\": \"precision\",\n",
    "               \"Recall\": \"recall\"\n",
    "               }\n",
    "    reg_cv = GridSearchCV(lr_clf, GSCV_parameters, cv=10, scoring=scoring, refit=\"F1_Score\", verbose=0)\n",
    "    reg_cv.fit(x_train, y_train)  # grid search CV makes division of dataset on test&train by itself\n",
    "\n",
    "    print(\"GridSearch mean results beetwere:\")\n",
    "    print(\"CROSS VALID FINISHED, RESULTS:\")\n",
    "    print(\"Best params: \", reg_cv.best_estimator_.get_params())\n",
    "\n",
    "    print(\"Best F1 Score result: \", reg_cv.best_score_)\n",
    "    print(\"Index of the best model is: \", np.argmax(np.nan_to_num(reg_cv.cv_results_[\"mean_test_F1_Score\"])))\n",
    "    print(\"Mean within K10Folds validations F1 Score result: \",\n",
    "          np.max(np.nan_to_num(reg_cv.cv_results_[\"mean_test_F1_Score\"])))\n",
    "    print(\"Mean within K10Folds validations Accuracy result: \",\n",
    "          np.max(np.nan_to_num(reg_cv.cv_results_[\"mean_test_Accuracy\"])))\n",
    "    print(\"Mean within K10Folds validations AUC result: \", np.max(np.nan_to_num(reg_cv.cv_results_[\"mean_test_AUC\"])))\n",
    "    print(\"Mean within K10Folds validations NegLogLoss result: \",\n",
    "          np.max(np.nan_to_num(reg_cv.cv_results_[\"mean_test_NegLogLoss\"])))\n",
    "    print(\"Mean within K10Folds validations Precision result: \",\n",
    "          np.max(np.nan_to_num(reg_cv.cv_results_[\"mean_test_Precision\"])))\n",
    "    print(\"Mean within K10Folds validations Recall result: \",\n",
    "          np.max(np.nan_to_num(reg_cv.cv_results_[\"mean_test_Recall\"])))\n",
    "    return reg_cv\n",
    "\n",
    "\n",
    "def eval_fun_br(x_test, y_test, classifier):\n",
    "    y_labels = np.array(classifier.predict(x_test))\n",
    "    conf_matrix = confusion_matrix(y_test.to_numpy(), y_labels)\n",
    "    f1 = f1_score(y_test, y_labels)\n",
    "    precision = precision_score(y_test, y_labels)\n",
    "    recall = recall_score(y_test, y_labels\n",
    "                          )\n",
    "    print(\"PREC: \", precision)\n",
    "    print(\"REC: \", recall)\n",
    "    print(\"F1 Score: \", f1)\n",
    "\n",
    "    return (pandas.DataFrame({\"Accuracy\": np.round(np.mean([accuracy_score(y_test.to_numpy(),\n",
    "                                                                           y_labels) for i in range(6)]), 3),\n",
    "                              \"F1\": np.round(f1, 3),\n",
    "                              \"Precision\": np.round(precision, 3),\n",
    "                              \"Recall\": np.round(recall, 3),\n",
    "                              \"AUC\": np.round(np.mean([roc_auc_score(y_test.to_numpy(),\n",
    "                                                                     y_labels) for i in range(6)]), 3),\n",
    "                              \"Log loss\": np.round(np.mean([log_loss(y_test.to_numpy(),\n",
    "                                                                     y_labels) for i in range(6)]), 3)}, index=[0]),\n",
    "            conf_matrix)\n",
    "\n",
    "\n",
    "def bert_compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "def bert_model(dataset, trd, tsd, model):\n",
    "    model_impl = RobertaForSequenceClassification.from_pretrained(model, num_labels=2)\n",
    "\n",
    "    targs = TrainingArguments(\n",
    "        output_dir=f\"./trained-{model}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=4e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model_impl,\n",
    "        args=targs,\n",
    "        train_dataset=trd,\n",
    "        eval_dataset=tsd,\n",
    "        compute_metrics=bert_compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    print(f\"MODEL \\\"{model}\\\" RESULTS\")\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    print(f\"Accuracy: {metrics['eval_accuracy']:.3f}\")\n",
    "    print(f\"Precision: {metrics['eval_precision']:.3f}\")\n",
    "    print(f\"Recall: {metrics['eval_recall']:.3f}\")\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"F1 Score: {metrics['eval_f1']:.3f}\")\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48dced4e-e632-4b40-acf0-3be1ec7db702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['message', 'is_toxic', '__index_level_0__'],\n",
      "    num_rows: 12494\n",
      "})\n",
      "['aa' 'aa ae' 'aa cc' 'aa reserve' 'aa review' 'aa service' 'aaargh'\n",
      " 'aaargh damn' 'aae' 'aae cfcbc']\n",
      "GridSearch mean results beetwere:\n",
      "CROSS VALID FINISHED, RESULTS:\n",
      "Best params:  {'C': 100, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "Best F1 Score result:  0.6825115351022284\n",
      "Index of the best model is:  14\n",
      "Mean within K10Folds validations F1 Score result:  0.6825115351022284\n",
      "Mean within K10Folds validations Accuracy result:  0.9013514514514513\n",
      "Mean within K10Folds validations AUC result:  0.9315673577968143\n",
      "Mean within K10Folds validations NegLogLoss result:  0.0\n",
      "Mean within K10Folds validations Precision result:  0.9390554232153455\n",
      "Mean within K10Folds validations Recall result:  0.5570680628272251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c10df58ed646d182af5a3c001e828e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9022341b4e410b87ac5dffbbb8b8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe52e9fa186c45d6ac1161649bd9f41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.216900</td>\n",
       "      <td>0.313110</td>\n",
       "      <td>0.871686</td>\n",
       "      <td>0.877638</td>\n",
       "      <td>0.889804</td>\n",
       "      <td>0.871686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1313' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 09:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL \"roberta-base\" RESULTS\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Accuracy: 0.872\n",
      "Precision: 0.890\n",
      "Recall: 0.872\n",
      "--------------------------\n",
      "F1 Score: 0.878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5b86b039dd4165815559fef4416420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ba5a1f8bb6437494c0ef82c7e25acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e69f7f97a94b629a14b18f848501b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.329692</td>\n",
       "      <td>0.872311</td>\n",
       "      <td>0.879345</td>\n",
       "      <td>0.895773</td>\n",
       "      <td>0.872311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1313' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL \"microsoft/codebert-base\" RESULTS\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Accuracy: 0.872\n",
      "Precision: 0.896\n",
      "Recall: 0.872\n",
      "--------------------------\n",
      "F1 Score: 0.879\n",
      "MODELS FULL COMPARE\n",
      "--------------------\n",
      "Best Logistic Regression\n",
      "PREC:  0.8303964757709251\n",
      "REC:  0.7495029821073559\n",
      "F1 Score:  0.7878787878787878\n",
      "(   Accuracy     F1  Precision  Recall    AUC  Log loss\n",
      "0     0.919  0.788       0.83    0.75  0.855     2.928, array([[1919,   77],\n",
      "       [ 126,  377]], dtype=int64))\n",
      "MODEL \"ROBERTA\" RESULTS\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Accuracy: 0.868\n",
      "Precision: 0.881\n",
      "Recall: 0.868\n",
      "F1 Score: 0.873\n",
      "--------------------------\n",
      "MODEL \"CODEBERT\" RESULTS\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Accuracy: 0.871\n",
      "Precision: 0.890\n",
      "Recall: 0.871\n",
      "F1 Score: 0.877\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "classifier(dataset, \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47556dc3-1045-478c-a29e-8a155f13fc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['message', 'is_toxic', '__index_level_0__'],\n",
      "    num_rows: 12494\n",
      "})\n",
      "['aa' 'aa ae' 'aa cc' 'aa reserve' 'aa review' 'aa service' 'aaargh'\n",
      " 'aaargh damn' 'aae' 'aae cfcbc']\n",
      "GridSearch mean results beetwere:\n",
      "CROSS VALID FINISHED, RESULTS:\n",
      "Best params:  {'C': 100, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "Best F1 Score result:  0.6825115351022284\n",
      "Index of the best model is:  14\n",
      "Mean within K10Folds validations F1 Score result:  0.6825115351022284\n",
      "Mean within K10Folds validations Accuracy result:  0.9013514514514513\n",
      "Mean within K10Folds validations AUC result:  0.9315673577968143\n",
      "Mean within K10Folds validations NegLogLoss result:  0.0\n",
      "Mean within K10Folds validations Precision result:  0.9390554232153455\n",
      "Mean within K10Folds validations Recall result:  0.5570680628272251\n",
      "PREC:  0.8303964757709251\n",
      "REC:  0.7495029821073559\n",
      "F1 Score:  0.7878787878787878\n",
      "{'tn': 1919, 'fp': 77, 'fn': 126, 'tp': 377}\n"
     ]
    }
   ],
   "source": [
    "classifier(dataset, \"classic_ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b649d8a3-0996-45ed-83f9-760b55550caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['message', 'is_toxic', '__index_level_0__'],\n",
      "    num_rows: 12494\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cba40581ea407ca2e572fe912c945c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7a8aafe185442c8b23d479e9177ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0388a62a884d3c9b85d760c63c0107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.193900</td>\n",
       "      <td>0.322474</td>\n",
       "      <td>0.863182</td>\n",
       "      <td>0.871600</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.863182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1313' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 02:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL \"roberta-base\" RESULTS\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Accuracy: 0.863\n",
      "Precision: 0.892\n",
      "Recall: 0.863\n",
      "--------------------------\n",
      "F1 Score: 0.872\n",
      "MODEL \"roberta-base\" VALIDATION RESULTS\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Accuracy: 0.864\n",
      "Precision: 0.890\n",
      "Recall: 0.864\n",
      "F1 Score: 0.871\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "classifier(dataset, \"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fad781c-90c7-4424-a8a7-8d5844dc7730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['message', 'is_toxic', '__index_level_0__'],\n",
      "    num_rows: 12494\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a9180ee76a4271bc58b1898d3e30f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf544d52292a4170b194be03ac392df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182e7bbaebba48b4b478d5029acc23b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>0.328968</td>\n",
       "      <td>0.869560</td>\n",
       "      <td>0.875886</td>\n",
       "      <td>0.889076</td>\n",
       "      <td>0.869560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1313' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 02:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL \"microsoft/codebert-base\" RESULTS\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Accuracy: 0.870\n",
      "Precision: 0.889\n",
      "Recall: 0.870\n",
      "--------------------------\n",
      "F1 Score: 0.876\n",
      "MODEL \"microsoft/codebert-base\" VALIDATION RESULTS\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Accuracy: 0.863\n",
      "Precision: 0.876\n",
      "Recall: 0.863\n",
      "F1 Score: 0.868\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "classifier(dataset, \"microsoft/codebert-base\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
